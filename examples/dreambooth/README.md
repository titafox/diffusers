# DreamBoothè®­ç»ƒç¤ºä¾‹

[DreamBooth](https://arxiv.org/abs/2208.12242)æ˜¯ä¸€ç§ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹çš„æ–¹æ³•,å¦‚stable diffusion,åªéœ€è¦æžå°‘é‡(3~5)çš„æŸä¸ªä¸»é¢˜çš„å›¾ç‰‡ã€‚

`train_dreambooth.py` è„šæœ¬å±•ç¤ºäº†å¦‚ä½•å®žçŽ°è®­ç»ƒè¿‡ç¨‹å¹¶å°†å…¶é€‚é…åˆ°stable diffusionã€‚

## åœ¨æœ¬åœ°ç”¨PyTorchè¿è¡Œ

### å®‰è£…ä¾èµ–

åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰,è¯·ç¡®ä¿å®‰è£…åº“çš„è®­ç»ƒä¾èµ–:

**é‡è¦**

ä¸ºäº†ç¡®ä¿ä½ å¯ä»¥æˆåŠŸè¿è¡Œæœ€æ–°ç‰ˆæœ¬çš„ç¤ºä¾‹è„šæœ¬,æˆ‘ä»¬å¼ºçƒˆå»ºè®®**ä»Žæºä»£ç å®‰è£…**å¹¶ä¿æŒå®‰è£…æ›´æ–°,å› ä¸ºæˆ‘ä»¬ç»å¸¸æ›´æ–°ç¤ºä¾‹è„šæœ¬å¹¶å®‰è£…ä¸€äº›ç¤ºä¾‹ç‰¹å®šçš„è¦æ±‚ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹,è¯·åœ¨ä¸€ä¸ªæ–°çš„è™šæ‹ŸçŽ¯å¢ƒä¸­æ‰§è¡Œä»¥ä¸‹æ­¥éª¤:
```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install -e .
```

Then cd in the example folder and run
ç„¶åŽcd åˆ°exampleæ–‡ä»¶å¤¹å¹¶run
```bash
pip install -r requirements.txt
```

å¹¶ç”¨ä»¥ä¸‹å‘½ä»¤åˆå§‹åŒ–ä¸€ä¸ª[ðŸ¤—Accelerate]([https://github.com/huggingface/accelerate/)çŽ¯å¢ƒ](https://github.com/huggingface/accelerate/)%E7%8E%AF%E5%A2%83):

```bash
accelerate config
```
æˆ–è€…ä»€ä¹ˆé—®é¢˜ä¸è¦å›žç­”ï¼Œç›´æŽ¥è®¾ç½®é»˜è®¤accelerateé…ç½®

```bash
accelerate config default
```

æˆ–è€…å¦‚æžœä½ çš„çŽ¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shell,ä¾‹å¦‚notebook

```python
from accelerate.utils import write_basic_config
write_basic_config()
```

ç¿»è¯‘:å½“è¿è¡Œ`accelerate config`æ—¶,å¦‚æžœæˆ‘ä»¬å°†torchç¼–è¯‘æ¨¡å¼è®¾ç½®ä¸ºTrue,å¯ä»¥æ˜Žæ˜¾åŠ é€Ÿã€‚

### ç‹—çŽ©å…·ç¤ºä¾‹

çŽ°åœ¨è®©æˆ‘ä»¬èŽ·å–æ•°æ®é›†ã€‚å¯¹äºŽè¿™ä¸ªç¤ºä¾‹,æˆ‘ä»¬å°†ä½¿ç”¨ä¸€äº›ç‹—çš„å›¾ç‰‡:[https://huggingface.co/datasets/diffusers/dog-exampleã€‚](https://huggingface.co/datasets/diffusers/dog-example%E3%80%82)

é¦–å…ˆè®©æˆ‘ä»¬åœ¨æœ¬åœ°ä¸‹è½½å®ƒ:

```python
from huggingface_hub import snapshot_download

local_dir = "./dog"
snapshot_download(
    "diffusers/dog-example",
    local_dir=local_dir, repo_type="dataset",
    ignore_patterns=".gitattributes",
)
```

ç„¶åŽä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ:

**___æ³¨æ„:å¦‚æžœä½ ä½¿ç”¨çš„æ˜¯ [stable-diffusion-2]([https://huggingface.co/stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)) 768x768 æ¨¡åž‹,è¯·å°† `resolution` æ”¹ä¸º768ã€‚___**

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export INSTANCE_DIR="dog"
export OUTPUT_DIR="path-to-save-model"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a photo of sks dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --learning_rate=5e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=400 \
  --push_to_hub
```

#### --push_to_hub

--push_to_hub æ˜¯å°†è®­ç»ƒå¥½çš„DreamBoothæ¨¡åž‹æŽ¨é€åˆ°Hugging Face Hubæ¥åˆ†äº«çš„é€‰é¡¹ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹:

1. åœ¨Hugging Faceä¸Šæ³¨å†Œè´¦å·,ç„¶åŽåœ¨ç½‘ç«™çš„Settingsä¸­èŽ·å–ä¸ªäººAccess Tokenã€‚
2. åœ¨è®­ç»ƒå‘½ä»¤æœ€åŽåŠ ä¸Š--push_to_hubå‚æ•°,åŒæ—¶ç”¨--hub_tokenæŒ‡å®šAccess Token:

```bash
--hub_token="xxxxxxx"
````
3.  å¯é€‰åœ°ç”¨--hub_model_idæŒ‡å®šæ¨¡åž‹åœ¨Hubä¸Šçš„åå­—,ä¸æŒ‡å®šä¼šä½¿ç”¨output_dirçš„åå­—:
```bash
--hub_model_id="titafox/test"
```
4. è®­ç»ƒç»“æŸåŽ,è„šæœ¬ä¼šè‡ªåŠ¨åˆ›å»ºHubä»“åº“,ä¸Šä¼ æ¨¡åž‹æ–‡ä»¶å’Œç”Ÿæˆç¤ºä¾‹å›¾ç‰‡ã€‚
5. åœ¨Hubä¸Šåˆ·æ–°ä½ çš„æ¨¡åž‹åˆ—è¡¨,å°±å¯ä»¥çœ‹åˆ°æ–°ä¸Šä¼ çš„DreamBoothæ¨¡åž‹,å¯ä»¥ç›´æŽ¥ä½¿ç”¨ã€‚

### ä½¿ç”¨å…ˆéªŒä¿ç•™æŸå¤±è¿›è¡Œè®­ç»ƒ

å…ˆéªŒä¿ç•™ç”¨äºŽé¿å…è¿‡æ‹Ÿåˆå’Œè¯­è¨€æ¼‚ç§»ã€‚å‚é˜…è®ºæ–‡ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚ å¯¹äºŽå…ˆéªŒä¿ç•™,æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ¨¡åž‹å’Œä¸€ä¸ªç±»åˆ«æç¤ºç”Ÿæˆå›¾åƒ,ç„¶åŽåœ¨è®­ç»ƒæ—¶å°†è¿™äº›å›¾åƒä¸Žæˆ‘ä»¬çš„æ•°æ®ä¸€èµ·ä½¿ç”¨ã€‚

æ ¹æ®è®ºæ–‡,å¯¹äºŽå…ˆéªŒä¿ç•™ç”Ÿæˆ `num_epochs * num_samples` ä¸ªå›¾åƒæ˜¯æŽ¨èçš„ã€‚å¯¹äºŽå¤§å¤šæ•°æƒ…å†µ,200-300å°±è¶³å¤Ÿäº†ã€‚`num_class_images` æ ‡å¿—è®¾ç½®ä½¿ç”¨ç±»åˆ«æç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚æ‚¨å¯ä»¥å°†çŽ°æœ‰å›¾åƒæ”¾åœ¨`class_data_dir`ä¸­,è®­ç»ƒè„šæœ¬å°†ç”Ÿæˆä»»ä½•é¢å¤–çš„å›¾åƒ,ä»¥ä¾¿åœ¨è®­ç»ƒæ—¶`class_data_dir`ä¸­å­˜åœ¨`num_class_images`ã€‚

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --learning_rate=5e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=200 \
  --max_train_steps=800 \
  --push_to_hub
```


### åœ¨16GB GPUä¸Šè®­ç»ƒ:

åœ¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ¥è‡ªbitsandbytesçš„8ä½ä¼˜åŒ–å™¨çš„å¸®åŠ©ä¸‹,å¯ä»¥åœ¨16GB GPUä¸Šè¿è¡Œtrain dreamboothã€‚

è¦å®‰è£…bitsandbytesè¯·å‚è€ƒè¿™ä¸ªè‡ªè¿°æ–‡ä»¶ã€‚

ä»¥ä¸‹æ˜¯åœ¨16GB GPUä¸Šè®­ç»ƒçš„ä¸€ä¸ªç¤ºä¾‹å‘½ä»¤:

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=2 --gradient_checkpointing \
  --use_8bit_adam \
  --learning_rate=5e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=200 \
  --max_train_steps=800 \
  --push_to_hub
```
å…³é”®æ˜¯ä½¿ç”¨ --mixed_precision="fp16" å’Œ --gradient_accumulation_steps=1 æ¥å‡å°‘GPUå†…å­˜ä½¿ç”¨ã€‚

### åœ¨12GB GPUä¸Šè®­ç»ƒ:

é€šè¿‡ä½¿ç”¨ä»¥ä¸‹ä¼˜åŒ–,å¯ä»¥åœ¨12GB GPUä¸Šè¿è¡Œdreambooth:

- [æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œ8ä½ä¼˜åŒ–å™¨](#åœ¨16GB-GPUä¸Šè®­ç»ƒ)

- [xformers](#ä½¿ç”¨xformersè®­ç»ƒ)

- [è®¾ç½®gradsä¸ºnone](#è®¾ç½®gradsä¸ºnone)

ä»¥ä¸‹æ˜¯ä¸€ä¸ª12GB GPUä¸Šçš„ç¤ºä¾‹å‘½ä»¤:

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 --gradient_checkpointing \
  --use_8bit_adam \
  --enable_xformers_memory_efficient_attention \
  --set_grads_to_none \
  --learning_rate=2e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=200 \
  --max_train_steps=800 \
  --push_to_hub
```
å…³é”®æ˜¯ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ã€è¾ƒä½Žçš„å­¦ä¹ çŽ‡å’Œxformersæ¥å‡å°‘GPUå†…å­˜ä½¿ç”¨ã€‚

### åœ¨ 8 GB GPU ä¸Šè®­ç»ƒ:

é€šè¿‡ä½¿ç”¨ [DeepSpeed](https://www.deepspeed.ai/),å¯ä»¥å°†ä¸€äº›å¼ é‡ä»Ž VRAM å¸è½½åˆ° CPU æˆ– NVME,ä»¥ä¾¿ç”¨æ›´å°‘çš„ VRAM è¿›è¡Œè®­ç»ƒã€‚

éœ€è¦åœ¨ `accelerate config` ä¸­å¯ç”¨ DeepSpeedã€‚åœ¨é…ç½®è¿‡ç¨‹ä¸­,å¯¹â€œæ‚¨æ˜¯å¦è¦ä½¿ç”¨ DeepSpeed?â€é—®ç­”â€œæ˜¯â€ã€‚ä½¿ç”¨ DeepSpeed ç¬¬2é˜¶æ®µ,fp16 æ··åˆç²¾åº¦å’Œå°†å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ° cpu,å¯ä»¥åœ¨å°äºŽ 8 GB VRAM çš„æ¡ä»¶ä¸‹è¿›è¡Œè®­ç»ƒ,ä»£ä»·æ˜¯éœ€è¦æ˜Žæ˜¾æ›´å¤šçš„ RAM(çº¦ 25 GB)ã€‚å‚è§[æ–‡æ¡£](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)ä»¥èŽ·å–æ›´å¤š DeepSpeed é…ç½®é€‰é¡¹ã€‚

å°†é»˜è®¤çš„ Adam ä¼˜åŒ–å™¨æ›´æ”¹ä¸º DeepSpeed çš„ Adam ç‰¹æ®Šç‰ˆæœ¬ `deepspeed.ops.adam.DeepSpeedCPUAdam` å¯ä»¥æ˜Žæ˜¾åŠ é€Ÿ,ä½†å¯ç”¨å®ƒéœ€è¦ä¸Ž pytorch ç›¸åŒç‰ˆæœ¬çš„ CUDA å·¥å…·é“¾ã€‚ç›®å‰ 8 æ¯”ç‰¹ä¼˜åŒ–å™¨ä¼¼ä¹Žä¸Ž DeepSpeed ä¸å…¼å®¹ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ª DeepSpeed ç¤ºä¾‹å‘½ä»¤:

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

accelerate launch --mixed_precision="fp16" train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --sample_batch_size=1 \
  --gradient_accumulation_steps=1 --gradient_checkpointing \
  --learning_rate=5e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=200 \
  --max_train_steps=800 \
  --push_to_hub
```
å…³é”®æ˜¯ä½¿ç”¨ DeepSpeed å’Œ fp16 æ··åˆç²¾åº¦æ¥æ˜¾è‘—å‡å°‘ VRAM ä½¿ç”¨ã€‚

### class_data_dirå’Œinstance_data_dir

instance_data_dirå’Œclass_data_dirä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯:

* instance_data_dir: åŒ…å«è¦è®­ç»ƒçš„ç‰¹å®šå®žä¾‹å¯¹è±¡çš„å›¾åƒ,ä¾‹å¦‚æŸä¸ªå…·ä½“çš„äººæˆ–è€…ç‹—ã€‚è¿™äº›å›¾åƒä¼šè®©æ¨¡åž‹å­¦ä¼šç”Ÿæˆè¯¥ç‰¹å®šå®žä¾‹ã€‚
* class_data_dir: åŒ…å«åŒä¸€ç±»å¯¹è±¡çš„é€šç”¨å›¾åƒ,ä¾‹å¦‚å„ç§ä¸åŒç‹—çš„å›¾ç‰‡ã€‚è¿™äº›å›¾åƒæä¾›ç±»çš„å…ˆéªŒçŸ¥è¯†,é¿å…æ¨¡åž‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šå®žä¾‹ã€‚

instance_data_diræ˜¯å¿…éœ€çš„,åŒ…å«è¦è®­ç»ƒçš„å®žä¾‹å¯¹è±¡ã€‚è¿™ä¸»è¦å†³å®šäº†æ¨¡åž‹èƒ½ç”Ÿæˆä»€ä¹ˆæ ·çš„å†…å®¹ã€‚

class_data_diræ˜¯å¯é€‰çš„,åŒ…å«æ›´å¤šåŒç±»åˆ«æ ·æœ¬ã€‚è¿™æœ‰åŠ©äºŽæ¨¡åž‹å­¦ä¹ æ•´ä¸ªç±»çš„ç‰¹å¾åˆ†å¸ƒ,é¿å…åªè®°ä½ç‰¹å®šå®žä¾‹çš„ä¿¡æ¯ã€‚

æ‰€ä»¥instance_data_diræ˜¯æ ¸å¿ƒæ•°æ®,class_data_diræ˜¯å¯é€‰çš„è¾…åŠ©æ•°æ®ã€‚formerå†³å®šäº†ç‰¹å®šç”Ÿæˆå†…å®¹,latterå¸®åŠ©æ¨¡åž‹å­¦ä¹ ç±»çš„å…ˆéªŒçŸ¥è¯†ã€‚äºŒè€…å…±åŒæé«˜ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ã€‚

ç®€å•æ¥è¯´,instance_data_diræ˜¯â€œè¿™ä¸ªâ€,class_data_diræ˜¯â€œè¿™ç±»ä¸œè¥¿â€ã€‚ä¸¤è€…å…±åŒä½œç”¨è®©æ¨¡åž‹æ—¢å­¦ä¼šç‰¹å®šå®žä¾‹,åˆä¸å¿˜è®°æ›´å¹¿æ³›çš„å…ˆéªŒçŸ¥è¯†ã€‚

#### instance_data_dir

instance_data_dirç›®å½•ä¸‹åº”è¯¥åŒ…å«ç”¨äºŽDreamBoothè®­ç»ƒçš„instanceå›¾åƒã€‚

è¯¥ç›®å½•çš„ç»„ç»‡ç»“æž„é€šå¸¸å¾ˆç®€å•,ç±»ä¼¼å¦‚ä¸‹:

* INSTANCE_DIR
  * img1.png
  * img2.png
  * img3.png
  * ...
ä¹Ÿå°±æ˜¯è¯´instance_data_dirç›´æŽ¥æ”¾ç½®instanceå¯¹è±¡(ä¾‹å¦‚ç‰¹å®šç‹—)çš„å„ç§å›¾ç‰‡ã€‚

å›¾åƒçš„æ•°é‡å’Œè´¨é‡ä¼šç›´æŽ¥å½±å“è®­ç»ƒæ•ˆæžœã€‚ä¸€èˆ¬æ¥è¯´,æ•°é‡è¶Šå¤šæ•ˆæžœè¶Šå¥½,ä½†å³ä½¿åªæœ‰å‡ å¼ å›¾åƒä¹Ÿèƒ½å·¥ä½œã€‚å›¾åƒè´¨é‡è¶Šé«˜è¶Šå¥½ã€‚

å¦‚æžœå›¾åƒä¸­åŒ…å«å¤æ‚çš„èƒŒæ™¯,å¯ä»¥è€ƒè™‘å…ˆè¿›è¡ŒæŠ å›¾æ¥èšç„¦å¯¹è±¡ã€‚

å¦å¤–,å¦‚æžœç›®æ ‡æ˜¯ç”Ÿæˆäººåƒ,ç”±äºŽéœ€è¦é¿å…ç”Ÿæˆä¸å­˜åœ¨çš„äºº,å®žä¾‹å›¾åƒä¸å®œè¿‡å°‘(ä¸€èˆ¬éœ€è¦10å¼ ä»¥ä¸Š)ã€‚

æ‰€ä»¥å®žä¾‹å›¾åƒç›®å½•å¾ˆç®€å•,ç›´æŽ¥å‡†å¤‡å¥½instanceå¯¹è±¡çš„å›¾ç‰‡å°±å¯ä»¥,æ•°é‡è´¨é‡è¶Šé«˜æ•ˆæžœè¶Šå¥½ã€‚è¿™æ˜¯DreamBoothè®­ç»ƒæœ€å…³é”®çš„æ•°æ®ã€‚

#### class_data_dir
class_data_dirç›®å½•ä¸‹åº”è¯¥åŒ…å«ä¸€ç³»åˆ—ç”¨äºŽè®­ç»ƒçš„ç±»å›¾åƒ,ç”¨äºŽæä¾›ç±»å…ˆéªŒä¿¡æ¯ã€‚

è¯¥ç›®å½•çš„ç»„ç»‡ç»“æž„é€šå¸¸å¦‚ä¸‹:

* class_data_dir
  * class1
    * img1.png
    * img2.png ...
  * class2
    * img1.png ...
  * ...

ä¹Ÿå°±æ˜¯è¯´,class_data_dirä¸‹åŒ…å«å¤šä¸ªç±»åˆ«çš„å­ç›®å½•,æ¯ä¸ªå­ç›®å½•ä¸‹æ˜¯å¯¹åº”ç±»åˆ«çš„å›¾åƒã€‚

å›¾åƒçš„æ•°é‡å’Œè´¨é‡ä¼šå½±å“æœ€ç»ˆçš„æ¨¡åž‹æ•ˆæžœã€‚ä¸€èˆ¬æ¥è¯´,æ¯ä¸ªç±»åˆ«è‡³å°‘éœ€è¦100å¼ ä»¥ä¸Šçš„é«˜è´¨é‡å›¾åƒ,æ‰èƒ½æä¾›è¶³å¤Ÿçš„å…ˆéªŒçŸ¥è¯†ã€‚

å¦‚æžœclass_data_dirä¸­çš„å›¾åƒä¸è¶³,è®­ç»ƒè„šæœ¬åœ¨è¿è¡Œæ—¶ä¼šè‡ªåŠ¨é‡‡æ ·ç”Ÿæˆæ›´å¤šç±»å›¾åƒä»¥è¾¾åˆ°æ‰€éœ€çš„æ•°é‡ã€‚

å¦å¤–,class_data_dirå¯ä»¥ä¸ºç©ºæˆ–è€…ä¸æä¾›,è¿™æ—¶å°±ä¸ä¼šä½¿ç”¨ç±»å…ˆéªŒçš„æŸå¤±,æ•ˆæžœå¯èƒ½ä¼šç•¥å·®ä¸€äº›ã€‚

æ‰€ä»¥åœ¨æœ‰é™çš„æ•°æ®æ¡ä»¶ä¸‹,æä¾›class_data_dirå¯ä»¥å¸®åŠ©æå‡æ¨¡åž‹å¯¹ç±»ç‰¹å¾çš„å­¦ä¹ ã€‚ä½†å³ä½¿æ²¡æœ‰ä¹Ÿå¯ä»¥è¿›è¡ŒDreamBoothå¾®è°ƒè®­ç»ƒã€‚


### ä½¿ç”¨UNetå¾®è°ƒæ–‡æœ¬ç¼–ç å™¨ã€‚

è¯¥è„šæœ¬ä¹Ÿå…è®¸ç»†è°ƒ`text_encoder`å’Œ`unet`ã€‚é€šè¿‡å®žéªŒè§‚å¯Ÿåˆ°,å¾®è°ƒ`text_encoder`å¯ä»¥èŽ·å¾—æ›´å¥½çš„ç»“æžœ,ç‰¹åˆ«æ˜¯åœ¨äººè„¸ä¸Šã€‚

ä¼ é€’`--train_text_encoder`å‚æ•°ç»™è„šæœ¬ä»¥å¯ç”¨è®­ç»ƒ`text_encoder`ã€‚

___æ³¨æ„:è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨éœ€è¦æ›´å¤šå†…å­˜,ä½¿ç”¨æ­¤é€‰é¡¹,è®­ç»ƒå°†ä¸é€‚åˆ16GB GPUã€‚å®ƒè‡³å°‘éœ€è¦24GB VRAMã€‚___

```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --train_text_encoder \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --use_8bit_adam \
  --gradient_checkpointing \
  --learning_rate=2e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=200 \
  --max_train_steps=800 \
  --push_to_hub
```

### å°†DreamBoothç”¨äºŽé™¤Stable Diffusionä¹‹å¤–çš„æµç¨‹

[AltDiffusionæµç¨‹](https://huggingface.co/docs/diffusers/api/pipelines/alt_diffusion)ä¹Ÿæ”¯æŒdreamboothå¾®è°ƒã€‚è¿‡ç¨‹ä¸Žä¸Šè¿°ç›¸åŒ,æ‚¨éœ€è¦åšçš„å°±æ˜¯åƒè¿™æ ·æ›¿æ¢`MODEL_NAME`:
```
export MODEL_NAME="CompVis/stable-diffusion-v1-4" --> export MODEL_NAME="BAAI/AltDiffusion-m9"
or
export MODEL_NAME="CompVis/stable-diffusion-v1-4" --> export MODEL_NAME="BAAI/AltDiffusion"
```

### æŽ¨ç†

ä¸€æ—¦ä½ ä½¿ç”¨ä¸Šè¿°å‘½ä»¤è®­ç»ƒäº†ä¸€ä¸ªæ¨¡åž‹,ä½ å¯ä»¥ç®€å•åœ°ä½¿ç”¨`StableDiffusionPipeline`è¿è¡ŒæŽ¨ç†ã€‚ç¡®ä¿åœ¨ä½ çš„æç¤ºä¸­åŒ…å«`identifier`(ä¾‹å¦‚ä¸Šä¾‹ä¸­çš„sks)ã€‚

```python
from diffusers import StableDiffusionPipeline
import torch

model_id = "path-to-your-trained-model"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

prompt = "A photo of sks dog in a bucket"
image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]

image.save("dog-bucket.png")
```

### æŽ¨ç†è®­ç»ƒæ£€æŸ¥ç‚¹

å¦‚æžœä½ ä½¿ç”¨äº†--checkpointing_stepså‚æ•°,ä¹Ÿå¯ä»¥ä»Žè®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æ£€æŸ¥ç‚¹ä¹‹ä¸€æ‰§è¡ŒæŽ¨ç†ã€‚è¯·å‚é˜…æ–‡æ¡£ä»¥æŸ¥çœ‹å¦‚ä½•æ“ä½œã€‚

ä½¿ç”¨å¤§è¯­è¨€æ¨¡åž‹çš„ä½Žç§©é€‚é…(LoRA)è¿›è¡Œè®­ç»ƒ
ä½Žç§©å¤§è¯­è¨€æ¨¡åž‹é€‚é…é¦–å…ˆç”±Microsoftåœ¨[LoRA:å¤§è¯­è¨€æ¨¡åž‹çš„ä½Žç§©é€‚é…](https://arxiv.org/abs/2106.09685)ä¸­æå‡º,ä½œè€…æ˜¯_Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen_

ç®€è€Œè¨€ä¹‹,LoRAå…è®¸é€šè¿‡å‘çŽ°æœ‰æƒé‡æ·»åŠ ç­‰çº§åˆ†è§£çŸ©é˜µå¯¹æ¥é€‚é…é¢„è®­ç»ƒæ¨¡åž‹,å¹¶ä¸”**åª**è®­ç»ƒæ–°æ·»åŠ çš„æƒé‡ã€‚è¿™æœ‰å‡ ä¸ªä¼˜ç‚¹:

- ä»¥å‰çš„é¢„è®­ç»ƒæƒé‡ä¿æŒå†»ç»“,æ‰€ä»¥æ¨¡åž‹ä¸å¤ªå®¹æ˜“[ç¾éš¾æ€§é—å¿˜]((https://www.pnas.org/doi/10.1073/pnas.1611835114))

- ç­‰çº§åˆ†è§£çŸ©é˜µçš„å‚æ•°æ˜Žæ˜¾å°‘äºŽåŽŸå§‹æ¨¡åž‹,è¿™æ„å‘³ç€è®­ç»ƒå¥½çš„LoRAæƒé‡å¾ˆå®¹æ˜“ä¾¿æºã€‚

- LoRAæ³¨æ„åŠ›å±‚å…è®¸é€šè¿‡scaleå‚æ•°æŽ§åˆ¶æ¨¡åž‹é€‚é…æ–°çš„è®­ç»ƒå›¾åƒçš„ç¨‹åº¦ã€‚

[cloneofsimo](https://github.com/cloneofsimo)æ˜¯é¦–æ¬¡å°è¯•åœ¨Stable Diffusionä¸­è¿›è¡ŒLoRAè®­ç»ƒçš„äºº

åœ¨æµè¡Œçš„lora GitHubåº“ä¸­ã€‚

è®­ç»ƒ
è®©æˆ‘ä»¬ä»Žä¸€ä¸ªç®€å•çš„ä¾‹å­å¼€å§‹ã€‚æˆ‘ä»¬å°†é‡ç”¨å‰ä¸€èŠ‚ä¸­çš„ç‹—ç¤ºä¾‹ã€‚

é¦–å…ˆ,æ‚¨éœ€è¦æŒ‰è¯´æ˜Žè®¾ç½®dreamboothè®­ç»ƒç¤ºä¾‹å®‰è£…éƒ¨åˆ†ã€‚

æŽ¥ä¸‹æ¥,è®©æˆ‘ä»¬ä¸‹è½½ç‹—çš„æ•°æ®é›†ã€‚ä»Žè¿™é‡Œä¸‹è½½å›¾åƒå¹¶ä¿å­˜åˆ°ä¸€ä¸ªç›®å½•ä¸­ã€‚è¯·ç¡®ä¿åœ¨ä¸‹é¢å°†INSTANCE_DIRè®¾ç½®ä¸ºç›®å½•çš„åç§°ã€‚è¿™å°†æ˜¯æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ã€‚

çŽ°åœ¨,æ‚¨å¯ä»¥å¯åŠ¨è®­ç»ƒã€‚è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨Stable Diffusion 1-5ã€‚

**___æ³¨æ„:å¦‚æžœæ‚¨ä½¿ç”¨çš„æ˜¯stable-diffusion-2 768x768 æ¨¡åž‹,è¯·å°†resolutionæ›´æ”¹ä¸º768ã€‚___**

**___æ³¨æ„:é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸç”Ÿæˆç¤ºä¾‹å›¾åƒæ¥ç›‘æŽ§è®­ç»ƒè¿›åº¦éžå¸¸æœ‰ç”¨ã€‚ wandbæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆ,å¯ä»¥è½»æ¾åœ°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°ç”Ÿæˆçš„å›¾åƒã€‚æ‚¨éœ€è¦åšçš„å°±æ˜¯åœ¨è®­ç»ƒå‰è¿è¡Œpip install wandb,å¹¶ä¼ é€’--report_to="wandb"è‡ªåŠ¨è®°å½•å›¾åƒã€‚___**


```bash
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export INSTANCE_DIR="dog"
export OUTPUT_DIR="path-to-save-model"
```

å¯¹äºŽè¿™ä¸ªä¾‹å­,æˆ‘ä»¬æƒ³ç›´æŽ¥åœ¨Hubä¸Šå­˜å‚¨è®­ç»ƒå¥½çš„LoRAåµŒå…¥,æ‰€ä»¥

æˆ‘ä»¬éœ€è¦ç™»å½•å¹¶æ·»åŠ  --push_to_hub å‚æ•°ã€‚

```bash
huggingface-cli login
```

å¦‚æžœæ˜¯åœ¨ colab ä¸Šä½ ä¹Ÿå¯ä»¥è¿™æ ·åšï¼š

```bash
!mkdir -p ~/.huggingface
HUGGINGFACE_TOKEN = "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" #@param {type:"string"}
!echo -n "{HUGGINGFACE_TOKEN}" > ~/.huggingface/token
```

çŽ°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒäº†

```bash
accelerate launch train_dreambooth_lora.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a photo of sks dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --checkpointing_steps=100 \
  --learning_rate=1e-4 \
  --report_to="wandb" \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=500 \
  --validation_prompt="A photo of sks dog in a bucket" \
  --validation_epochs=50 \
  --seed="0" \
  --push_to_hub
```

**___æ³¨æ„:åœ¨ä½¿ç”¨LoRAæ—¶,æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¯”æ™®é€šæ¢¦ä¹‹è¿¹æ›´é«˜çš„å­¦ä¹ çŽ‡ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨_1e-4_,è€Œä¸æ˜¯é€šå¸¸çš„_2e-6_ã€‚___**

æœ€ç»ˆçš„LoRAåµŒå…¥æƒé‡å·²ä¸Šä¼ åˆ°[patrickvonplaten/lora_dreambooth_dog_example]([https://huggingface.co/patrickvonplaten/lora\\_dreambooth\\_dog\\_example)ã€‚](https://huggingface.co/patrickvonplaten/lora%5C%5C_dreambooth%5C%5C_dog%5C%5C_example)%E3%80%82) **___æ³¨æ„:[æœ€ç»ˆçš„æƒé‡]([https://huggingface.co/patrickvonplaten/lora/blob/main/pytorch\\_attn\\_procs.bin)åªæœ‰3](https://huggingface.co/patrickvonplaten/lora/blob/main/pytorch%5C%5C_attn%5C%5C_procs.bin)%E5%8F%AA%E6%9C%893) MBå¤§å°,æ¯”åŽŸå§‹æ¨¡åž‹å°äº†å‡ ä¸ªæ•°é‡çº§ã€‚**

è®­ç»ƒç»“æžœæ€»ç»“[åœ¨è¿™é‡Œ](https://api.wandb.ai/report/patrickvonplaten/xm6cd5q5)ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨`Step`æ»‘å—æ¥æŸ¥çœ‹æ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•å­¦ä¹ æˆ‘ä»¬ä¸»é¢˜çš„ç‰¹å¾ã€‚

å¯é€‰åœ°,æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ºæ–‡æœ¬ç¼–ç å™¨è®­ç»ƒé¢å¤–çš„LoRAå±‚ã€‚ä¸ºæ­¤,è¯·æŒ‡å®šä¸Šè¿°çš„`--train_text_encoder`å‚æ•°ã€‚å¦‚æžœæ‚¨æœ‰å…´è¶£äº†è§£æˆ‘ä»¬å¦‚ä½•

å¯ç”¨æ­¤æ”¯æŒ,è¯·æŸ¥çœ‹æ­¤[PR](https://github.com/huggingface/diffusers/pull/2918)ã€‚

ä½¿ç”¨ä¸Šè¿°çš„é»˜è®¤è¶…å‚æ•°,è®­ç»ƒä¼¼ä¹Žæ­£åœ¨æœç€æ­£é¢æ–¹å‘å‘å±•ã€‚æŸ¥çœ‹[è¿™ä¸ªé¢æ¿](https://wandb.ai/sayakpaul/dreambooth-lora/reports/test-23-04-17-17-00-13---Vmlldzo0MDkwNjMy)ã€‚è®­ç»ƒå¥½çš„LoRAå±‚å¯åœ¨[è¿™é‡Œ](https://huggingface.co/sayakpaul/dreambooth)èŽ·å¾—ã€‚


### æŽ¨ç†

è®­ç»ƒåŽ,LoRAæƒé‡å¯ä»¥å¾ˆå®¹æ˜“åœ°åŠ è½½åˆ°åŽŸå§‹ç®¡é“ä¸­ã€‚é¦–å…ˆ,æ‚¨éœ€è¦åŠ è½½åŽŸå§‹ç®¡é“:

```python
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
import torch

pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe.to("cuda")
```

æŽ¥ä¸‹æ¥,æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[`load_attn_procs`å‡½æ•°](https://huggingface.co/docs/diffusers/api/loaders#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs)å°†é€‚é…å™¨å±‚åŠ è½½åˆ°UNetä¸­ã€‚

```python
pipe.unet.load_attn_procs("patrickvonplaten/lora_dreambooth_dog_example")
```

æœ€åŽ,æˆ‘ä»¬å¯ä»¥åœ¨æŽ¨ç†ä¸­è¿è¡Œè¯¥æ¨¡åž‹ã€‚

```python
image = pipe("A picture of a sks dog in a bucket", num_inference_steps=25).images[0]
```

å¦‚æžœä½ ä»ŽHubåŠ è½½LoRAå‚æ•°,å¹¶ä¸”Hubä»“åº“æœ‰ä¸€ä¸ª
`base_model` æ ‡ç­¾(æ¯”å¦‚ [è¿™ä¸ª](https://huggingface.co/patrickvonplaten/lora_dreambooth_dog_example/blob/main/README.md?code=true#L4)),é‚£ä¹ˆ
ä½ å¯ä»¥è¿™æ ·åš:

```py 
from huggingface_hub.repocard import RepoCard

lora_model_id = "patrickvonplaten/lora_dreambooth_dog_example"
card = RepoCard.load(lora_model_id)
base_model_id = card.data.to_dict()["base_model"]

pipe = StableDiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)
...
```

å¦‚æžœä½ åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨äº† --train_text_encoder,é‚£ä¹ˆä½¿ç”¨ pipe.load_lora_weights() æ¥åŠ è½½LoRAæƒé‡ã€‚ä¾‹å¦‚:

```python
from huggingface_hub.repocard import RepoCard
from diffusers import StableDiffusionPipeline
import torch 

lora_model_id = "sayakpaul/dreambooth-text-encoder-test"
card = RepoCard.load(lora_model_id)
base_model_id = card.data.to_dict()["base_model"]

pipe = StableDiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")
pipe.load_lora_weights(lora_model_id)
image = pipe("A picture of a sks dog in a bucket", num_inference_steps=25).images[0]
```

è¯·æ³¨æ„,ä¸Ž [`\UNet2DConditionLoadersMixin.load_attn_procs\`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs) ç›¸æ¯”,[`\LoraLoaderMixin.load_lora_weights\`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.load_lora_weights)æ˜¯åŠ è½½LoRAå‚æ•°çš„é¦–é€‰æ–¹æ³•ã€‚è¿™æ˜¯å› ä¸º`LoraLoaderMixin.load_lora_weights` å¯ä»¥å¤„ç†ä»¥ä¸‹æƒ…å†µ:

* æ²¡æœ‰å•ç‹¬æ ‡è¯†ç¬¦çš„LoRAå‚æ•°,ç”¨äºŽUNetå’Œæ–‡æœ¬ç¼–ç å™¨(æ¯”å¦‚ [`\patrickvonplaten/lora_dreambooth_dog_example\`](https://huggingface.co/patrickvonplaten/lora_dreambooth_dog_example))ã€‚æ‰€ä»¥,ä½ å¯ä»¥è¿™æ ·åš:

  ```py 
  pipe.load_lora_weights(lora_model_path)
  ```
  å®ƒå°†è‡ªåŠ¨åŠ è½½æ–‡æœ¬ç¼–ç å™¨å’ŒUNetçš„LoRAå‚æ•°ã€‚

* å¯¹äºŽåªè®­ç»ƒäº†æ–‡æœ¬ç¼–ç å™¨æˆ–UNetçš„æƒ…å†µ,å®ƒä¹Ÿå¯ä»¥æ­£å¸¸å·¥ä½œã€‚

æ‰€ä»¥æ€»çš„æ¥è¯´,LoraLoaderMixin.load_lora_weightsæä¾›äº†ä¸€ä¸ªæ ‡å‡†çš„APIæ¥åŠ è½½LoRAå‚æ•°,æ— éœ€æ‹…å¿ƒå®ƒä»¬çš„ç¡®åˆ‡ç»„ç»‡æ–¹å¼ã€‚

* å…·æœ‰UNetå’Œæ–‡æœ¬ç¼–ç å™¨å•ç‹¬æ ‡è¯†ç¬¦çš„LoRAå‚æ•°,ä¾‹å¦‚:  [`"sayakpaul/dreambooth"`](https://huggingface.co/sayakpaul/dreambooth).

## Training with Flax/JAX

For faster training on TPUs and GPUs you can leverage the flax training example. Follow the instructions above to get the model and dataset before running the script.

____Note: The flax example don't yet support features like gradient checkpoint, gradient accumulation etc, so to use flax for faster training we will need >30GB cards.___


Before running the scripts, make sure to install the library's training dependencies:

```bash
pip install -U -r requirements_flax.txt
```


### Training without prior preservation loss

```bash
export MODEL_NAME="duongna/stable-diffusion-v1-4-flax"
export INSTANCE_DIR="dog"
export OUTPUT_DIR="path-to-save-model"

python train_dreambooth_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a photo of sks dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --learning_rate=5e-6 \
  --max_train_steps=400
```


### Training with prior preservation loss

```bash
export MODEL_NAME="duongna/stable-diffusion-v1-4-flax"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

python train_dreambooth_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --learning_rate=5e-6 \
  --num_class_images=200 \
  --max_train_steps=800
```


### Fine-tune text encoder with the UNet.

```bash
export MODEL_NAME="duongna/stable-diffusion-v1-4-flax"
export INSTANCE_DIR="dog"
export CLASS_DIR="path-to-class-images"
export OUTPUT_DIR="path-to-save-model"

python train_dreambooth_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --train_text_encoder \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="a photo of sks dog" \
  --class_prompt="a photo of dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --learning_rate=2e-6 \
  --num_class_images=200 \
  --max_train_steps=800
```

### Training with xformers:
You can enable memory efficient attention by [installing xFormers](https://github.com/facebookresearch/xformers#installing-xformers) and padding the `--enable_xformers_memory_efficient_attention` argument to the script. This is not available with the Flax/JAX implementation.

You can also use Dreambooth to train the specialized in-painting model. See [the script in the research folder for details](https://github.com/huggingface/diffusers/tree/main/examples/research_projects/dreambooth_inpaint).

### Set grads to none

To save even more memory, pass the `--set_grads_to_none` argument to the script. This will set grads to None instead of zero. However, be aware that it changes certain behaviors, so if you start experiencing any problems, remove this argument.

More info: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html

### Experimental results
You can refer to [this blog post](https://huggingface.co/blog/dreambooth) that discusses some of DreamBooth experiments in detail. Specifically, it recommends a set of DreamBooth-specific tips and tricks that we have found to work well for a variety of subjects. 

## IF

You can use the lora and full dreambooth scripts to train the text to image [IF model](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) and the stage II upscaler 
[IF model](https://huggingface.co/DeepFloyd/IF-II-L-v1.0).

Note that IF has a predicted variance, and our finetuning scripts only train the models predicted error, so for finetuned IF models we switch to a fixed
variance schedule. The full finetuning scripts will update the scheduler config for the full saved model. However, when loading saved LoRA weights, you
must also update the pipeline's scheduler config.

```py
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("DeepFloyd/IF-I-XL-v1.0")

pipe.load_lora_weights("<lora weights path>")

# Update scheduler config to fixed variance schedule
pipe.scheduler = pipe.scheduler.__class__.from_config(pipe.scheduler.config, variance_type="fixed_small")
```

Additionally, a few alternative cli flags are needed for IF.

`--resolution=64`: IF is a pixel space diffusion model. In order to operate on un-compressed pixels, the input images are of a much smaller resolution. 

`--pre_compute_text_embeddings`: IF uses [T5](https://huggingface.co/docs/transformers/model_doc/t5) for its text encoder. In order to save GPU memory, we pre compute all text embeddings and then de-allocate
T5.

`--tokenizer_max_length=77`: T5 has a longer default text length, but the default IF encoding procedure uses a smaller number.

`--text_encoder_use_attention_mask`: T5 passes the attention mask to the text encoder.

### Tips and Tricks
We find LoRA to be sufficient for finetuning the stage I model as the low resolution of the model makes representing finegrained detail hard regardless.

For common and/or not-visually complex object concepts, you can get away with not-finetuning the upscaler. Just be sure to adjust the prompt passed to the
upscaler to remove the new token from the instance prompt. I.e. if your stage I prompt is "a sks dog", use "a dog" for your stage II prompt.

For finegrained detail like faces that aren't present in the original training set, we find that full finetuning of the stage II upscaler is better than 
LoRA finetuning stage II.

For finegrained detail like faces, we find that lower learning rates along with larger batch sizes work best.

For stage II, we find that lower learning rates are also needed.

We found experimentally that the DDPM scheduler with the default larger number of denoising steps to sometimes work better than the DPM Solver scheduler
used in the training scripts.

### Stage II additional validation images

The stage II validation requires images to upscale, we can download a downsized version of the training set:

```py
from huggingface_hub import snapshot_download

local_dir = "./dog_downsized"
snapshot_download(
    "diffusers/dog-example-downsized",
    local_dir=local_dir,
    repo_type="dataset",
    ignore_patterns=".gitattributes",
)
```

### IF stage I LoRA Dreambooth
This training configuration requires ~28 GB VRAM.

```sh
export MODEL_NAME="DeepFloyd/IF-I-XL-v1.0"
export INSTANCE_DIR="dog"
export OUTPUT_DIR="dreambooth_dog_lora"

accelerate launch train_dreambooth_lora.py \
  --report_to wandb \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a sks dog" \
  --resolution=64 \
  --train_batch_size=4 \
  --gradient_accumulation_steps=1 \
  --learning_rate=5e-6 \
  --scale_lr \
  --max_train_steps=1200 \
  --validation_prompt="a sks dog" \
  --validation_epochs=25 \
  --checkpointing_steps=100 \
  --pre_compute_text_embeddings \
  --tokenizer_max_length=77 \
  --text_encoder_use_attention_mask
```

### IF stage II LoRA Dreambooth

`--validation_images`: These images are upscaled during validation steps.

`--class_labels_conditioning=timesteps`: Pass additional conditioning to the UNet needed for stage II.

`--learning_rate=1e-6`: Lower learning rate than stage I.

`--resolution=256`: The upscaler expects higher resolution inputs

```sh
export MODEL_NAME="DeepFloyd/IF-II-L-v1.0"
export INSTANCE_DIR="dog"
export OUTPUT_DIR="dreambooth_dog_upscale"
export VALIDATION_IMAGES="dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png"

python train_dreambooth_lora.py \
    --report_to wandb \
    --pretrained_model_name_or_path=$MODEL_NAME \
    --instance_data_dir=$INSTANCE_DIR \
    --output_dir=$OUTPUT_DIR \
    --instance_prompt="a sks dog" \
    --resolution=256 \
    --train_batch_size=4 \
    --gradient_accumulation_steps=1 \
    --learning_rate=1e-6 \ 
    --max_train_steps=2000 \
    --validation_prompt="a sks dog" \
    --validation_epochs=100 \
    --checkpointing_steps=500 \
    --pre_compute_text_embeddings \
    --tokenizer_max_length=77 \
    --text_encoder_use_attention_mask \
    --validation_images $VALIDATION_IMAGES \
    --class_labels_conditioning=timesteps
```

### IF Stage I Full Dreambooth
`--skip_save_text_encoder`: When training the full model, this will skip saving the entire T5 with the finetuned model. You can still load the pipeline
with a T5 loaded from the original model.

`use_8bit_adam`: Due to the size of the optimizer states, we recommend training the full XL IF model with 8bit adam. 

`--learning_rate=1e-7`: For full dreambooth, IF requires very low learning rates. With higher learning rates model quality will degrade. Note that it is 
likely the learning rate can be increased with larger batch sizes.

Using 8bit adam and a batch size of 4, the model can be trained in ~48 GB VRAM.

```sh
export MODEL_NAME="DeepFloyd/IF-I-XL-v1.0"

export INSTANCE_DIR="dog"
export OUTPUT_DIR="dreambooth_if"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a photo of sks dog" \
  --resolution=64 \
  --train_batch_size=4 \
  --gradient_accumulation_steps=1 \
  --learning_rate=1e-7 \
  --max_train_steps=150 \
  --validation_prompt "a photo of sks dog" \
  --validation_steps 25 \
  --text_encoder_use_attention_mask \
  --tokenizer_max_length 77 \
  --pre_compute_text_embeddings \
  --use_8bit_adam \
  --set_grads_to_none \
  --skip_save_text_encoder \
  --push_to_hub
```

### IF Stage II Full Dreambooth

`--learning_rate=5e-6`: With a smaller effective batch size of 4, we found that we required learning rates as low as
1e-8.

`--resolution=256`: The upscaler expects higher resolution inputs

`--train_batch_size=2` and `--gradient_accumulation_steps=6`: We found that full training of stage II particularly with
faces required large effective batch sizes.

```sh
export MODEL_NAME="DeepFloyd/IF-II-L-v1.0"
export INSTANCE_DIR="dog"
export OUTPUT_DIR="dreambooth_dog_upscale"
export VALIDATION_IMAGES="dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png"

accelerate launch train_dreambooth.py \
  --report_to wandb \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a sks dog" \
  --resolution=256 \
  --train_batch_size=2 \
  --gradient_accumulation_steps=6 \
  --learning_rate=5e-6 \
  --max_train_steps=2000 \
  --validation_prompt="a sks dog" \
  --validation_steps=150 \
  --checkpointing_steps=500 \
  --pre_compute_text_embeddings \
  --tokenizer_max_length=77 \
  --text_encoder_use_attention_mask \
  --validation_images $VALIDATION_IMAGES \
  --class_labels_conditioning timesteps \
  --push_to_hub
```

## Stable Diffusion XL

We support fine-tuning of the UNet shipped in [Stable Diffusion XL](https://huggingface.co/papers/2307.01952) with DreamBooth and LoRA via the `train_dreambooth_lora_sdxl.py` script. Please refer to the docs [here](./README_sdxl.md). 
